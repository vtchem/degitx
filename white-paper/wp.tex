\documentclass[acmlarge, screen, nonacm]{acmart}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{setspace}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage[absolute]{textpos}\TPGrid{16}{16}
\usepackage{tikz}
  \usetikzlibrary{shapes}
  \usetikzlibrary{arrows.meta}
  \usetikzlibrary{arrows}
  \usetikzlibrary{shadows}
  \usetikzlibrary{trees}
  \usetikzlibrary{fit}
  \usetikzlibrary{calc}
  \usetikzlibrary{positioning}
  \usetikzlibrary{decorations.pathmorphing}
\usepackage{./tikz-uml}
\usepackage{xcolor}
\usepackage{hyperref}
  \hypersetup{colorlinks=true,allcolors=blue!40!black}
\setlength{\topskip}{6pt}
\setlength{\parindent}{0pt} % indent first line
\setlength{\parskip}{6pt} % before par
\date{\small\today}
\title[DeGit white paper]{[DRAFT] White-paper: DeGit - distributed git repository manager}
\definecolor{tikz-block}{HTML}{232527}
\tikzset{node distance=1.6cm, auto, every text node part/.style={align=center, font={\sffamily\small}}}
\tikzstyle{block} = [draw=tikz-block, fill=white, inner sep=0.3cm, outer sep=0.1cm, thick]
\tikzstyle{ln} = [draw, ->, very thick, arrows={-triangle 90}, every text node part/.append style={font={\sffamily\scriptsize}}]
\tikzstyle{O} = [circle, draw, every text node part/.style={align=center, font={\sffamily\small}}]

% custom commands
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}

\author{Kirill Chernyavskiy}
\email{g4s8.public@gmail.com}

\acmBooktitle{none}
\acmConference{none}
\editor{none}

\begin{document}
\raggedbottom

\begin{abstract}
  Big software companies or open source communities 
  may have millions of code repositories,
  and use them extensively by programmers and continuous integration (CI) pipelines.
  One git server is not able to satisfy performance expectations,
  many servers with load-balancing can't solve this issue too because
  of inability storage IO scaling for read operations.
  Also, a big company may have distributed teams around the world,
  where each team collaborates with others in one git repo,
  cross-region repo access could be slow in such cases.
  The solution for this problem is distributed git repository storage,
  which replicates repositories across region nodes.
\end{abstract}

\maketitle

\section{Introduction}

\todo{Add introduction}

\section{Technical problems}


There are three main probles to solve for distributed git repository manager (DGRM):
\begin{description}
\item[scalability] --- one repository may be used actively by many users simultaneously. Storage disk
  has input output (IO) operarion limits, and can't perform a lot of \code{fetch} operations in
  short period of time. The git repository storage should be replicated on different system nodes to
  solve this problem.
\item[dispatching] --- this user doesn't know network address of git repository storages. Also, according to the
  previous problem, one repository can be located on multiple replicas.
  So the system should be able to locate and redirect user's requests
  from git client\footnote{Git client is a software that performs git fetch and push operations, it can be
  command line tool, IDE or any graphical user interface communicating with DGRM} to correct git repository
  storage. It should lookup for repository nodes in system and load-balance requests to different storage replicas.
\item[consistency] --- the user may \code{push} to repository and \code{fetch} then, in that case it must receive
  same or newer data that user \code{push}ed previously, even if user \code{fetch}ed data from another
  replica of this repository. The linearizability of each single repository in a system is a must have option.
\end{description}

\subsection{Scalability}

Big software companies or big developer's communities have
millions of repositories distributed around the world, and big teams located in different countries
(regions). On high load of \code{fetch} requests git repository storage disks may go above IO operation limits.
Usually, repositories are accessed by different ways:
\begin{description}
  \item[Developer personal activity] --- programmers access git repositories via
    \code{git} client and performs \code{fetch}/\code{push} actions. Also, they
    may use web UI interface to upload files, edit files in browser, merge pull requests, etc.
    The number of requests for this activity is not very big according to collected statistics.
  \item[API access] --- a lot of services depends on repository management services. It's API robots
    collecting developers activity, background code analyzers of repository, IDEs communicating
    with repositories and reading some history data. The number of such requests is about
    few thousands requests per minute for each million of repositories.
  \item[CI systems] --- many events may trigger CI worfkflow, most coommon events are:
    new commit pushed, new pull (merge) request created, new tag pushed, etc.
    On each action CI system downloads the whole repository to run some workflows,
    the download (\code{clone}) operation accompanied by huge bandwidth consumption.
    In additional, some repositories may include submodules, it leads to submodules cloning by CI
    system. According to the statistic, the number is measured as ten thousands requests per minute
    for each million of repositories.
\end{description}

Event if git repository storage is distributed and clients are routed to storage with
correct git repository, the huge amount of \code{fetch} traffic for one repository still able to
make disk go above input output operations (IOP) limits and repository storage will stop
serving requests.

The scalability problem was tried to fix by asynchronous replication of git repository data:
there was proof of concept DGRM with eventually consistency guarantee;
one repository was replicated asynchronously after any update operation to multiple git repository storages,
and clients were load balanced to different git repository storages.
But it was not succeed because of two reasons:
\begin{enumerate}
  \item fetch traffic correlated with push frequency because of huge amount of
  CI systems and API robots involved in development process: each update event usually triggers
  CI build immediately, and CI clones the repository. CI was able to clone only
  primary repository (repository with actual data) because the replication has not been completed
  before CI cloning started.
  \item \code{push} and \code{fetch} frequency was not distributed uniformly over the time --- in each
  repository team members may have different responsibilities for review and merge process,
  project technical lead can merge all approved pull-requests in short period of time
  which causes frequent push operations in git repo.
\end{enumerate}

These two conditions lead to high fetch traffic peaks for git repositories:
frequent push operations turn replication storages into inconsistent state,
and lead to high fetch traffic to primary repository storages from CI systems which clones these repositories.
It causes the same problems as were common for non-replicated system --- the primary node goes beyond IOPS limits and
rejects new \code{fetcj} requests.

Futhermore, \code{merge} is not the only way to ``update'' the repository. Lots of other activities can do that,
here is the list of some:

\begin{description}
  \item[branches changing] \verb|create|/\verb|delete|/\verb|update| branches by git push or by using web page.
  \item[tags changing] \verb|create|/\verb|delete|/\verb|update| tags by git push or by releasing new version on web page.
  \item[special refs changing] when new merge request is created, a new\\
    \emph{\code{refs/merge-requests/IID\footnote{Internal id of a project on gitlab}/head}} named ref
    is created. When source branch of the merge request is updated, the ref is also updated.
  \item[migrations] sometimes repository administrator can migrate the repository to
    another physical device or another region node, it also could be treated as an update.
    Also, delete a repository could be treated as migrate it to a trash area.
\end{description}

\subsection{Dispatching}

Git client doesn't know where repository replica is located in the system. Dispatching algorithm
should be able to locate correct storage nodes (distributed system nodes) of repository and redirect
each client's request to one of these nodes. Moreover, it's not enough just to redirect client's request,
the dispatching endpoint should load-balance all requests for each particular repository to distribute
storage load over the time. Round-robin load-balancing solves this problem, since the scalability problems
occurs only on peeks of high read traffic, so statistically redirecting each next read request to
different storage node reduces disk load by \code{n}-times, where \code{n} is amount of storage replicas.

\subsection{Consistency}

\todo{Describe the problem}

% \subsection{Government restrictions}
% The most popular repository management systems are \href{https://github.com/}{GitHub} and
% \href{https://about.gitlab.com}{GitLab} are under control of government, these servcice may
% restric access for some groups of people based on their location and may reject to provide paid
% versions for hosting internal servers. \todo{add more details}.

\section{Solution}

The solution is a distributed Git repository manager with strong consistent replica nodes.
It consists of two parts: the back-end and front-end (see Figure ~\ref{fig:comp-arc-overview} diagram):

\begin{description}
  \item[The back end] --- (core network, storage), P2P\footnote{peer to peer} system which stores git data
    and metadata\footnote{Here git data referenced to git objects, and git metadata to git references.
    In this document git data usually refer to both objects and references unless otherwise stated},
    it's responsible for replication of repositories, and guarantee
    strong consistency of git storage across replicas. It exposes internal RPC API to accept git intermediate
    requests, which can modify repository state. And provide endpoints to read (\code{fetch}) repository data.
  \item[The front end] --- (multiplexer, load balancer). It exposes public API for all git operations,
    translates all operations request into intermediate RPC language, routes requests to proper storage
    nodes and load-balancing read (\code{fetch}) requests to different replicas of same repository.
\end{description}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \umlbasiccomponent[x=-3.5]{Client}

      \begin{umlcomponent}[x=1, y=-4]{Front-end}
        \umlbasiccomponent{Load-balancer}
        \umlprovidedinterface[interface=Entry point, name=lb-e6ce, padding=2cm]{Load-balancer}
        \umlbasiccomponent[y=-3]{Routing table}
      \end{umlcomponent}
      \umlport{Front-end}{west}
      \umldelegateconnector{Front-end-west-port}{lb-e6ce}
      \umlHVHassemblyconnector[interface=RPC, with port, name=fe-rpc,
        middle arm, arm1=-1cm, anchor1=-180]{Client}{Front-end}

      \begin{umlcomponent}[x=6, y=-1]{Back-end}
        \umlbasiccomponent{git storage}
      \end{umlcomponent}
      \umlHVassemblyconnector[interface=RPC, name=be-rpc, with port]{Front-end}{Back-end}

      \umlnote[x=1.2]{fe-rpc-interface}{Front-end exposes public API for git operations}
      \umlnote[x=7, y=-7]{be-rpc-interface}{Back-end exposes internal API for storage operations}
    \end{tikzpicture}
  \end{center}
  \caption{
    Components architecture overview:
    Each client C connected to DeGit front-end component,
    the front-end has load-balancer to proxy incoming requests to back-end,
    it finds back-end nodes using routing table.
  }\label{fig:comp-arc-overview}
\end{figure}

The front end exposes public API for well-known protocol, e.g. one front-end implementation provides
\href{https://docs.gitlab.com/ee/administration/gitaly/}{Gitaly} gRPC interfaces for GitLab
instance set\footnote{In this document GitLab instance set is a set of shell, workhorce and web components of GitLab},
see Figure ~\ref{fig:gitlab-set} diagram. There GitLab is a client of DeGitX front-end, it sends gRPC requests to
front-end to modify and fetch git data.

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \umlbasiccomponent[name=git, y=-0.5]{git CLI}
      \umlbasiccomponent[name=browser, y=-5]{Browser}

      \begin{umlcomponent}[name=gitlab, x=8]{GitLab}
        \umlbasiccomponent[name=gl-shell]{shell}
        \umlprovidedinterface[interface=SSH, name=gl-shell-ssh, padding=2cm, distance=2.5cm]{gl-shell}

        \umlbasiccomponent[name=gl-workhorse, y=-3]{Workhorse}
        \umlprovidedinterface[interface=HTTP, name=gl-wh-http, padding=2cm, distance=2.5cm]{gl-workhorse}

        \umlbasiccomponent[name=gl-web, y=-6]{Web}
        \umlprovidedinterface[interface=HTTP, name=gl-web-http, padding=2cm, distance=2.5cm]{gl-web}
      \end{umlcomponent}
      \umlport{gitlab}{150}
      \umldelegateconnector{gitlab-150-port}{gl-shell-ssh}
      \umlport{gitlab}{210}
      \umldelegateconnector{gitlab-210-port}{gl-wh-http}
      \umldelegateconnector{gitlab-210-port}{gl-web-http}
      \umlassemblyconnector{git}{gitlab-150-port}
      \umlassemblyconnector{git}{gitlab-210-port}
      \umlassemblyconnector{browser}{gitlab-210-port}
      \umlport{gitlab}{east}

      \umlbasiccomponent[name=gitaly-fe, x=8, y=-10]{DeGitX front-end}
      \umlport{gitaly-fe}{east}
      \umlHVHassemblyconnector[interface=Gitaly gRPC, arm1=1cm]{gitlab-east-port}{gitaly-fe-east-port}
    \end{tikzpicture}
  \end{center}
  \caption{
    GitLab set and Gitaly front-end:
    git client communicates with GitLab-shell via SSH or with GitLab-workhorse via HTTP(S).
    Browser uses GitLab-web. All three GitLab components communicates with git storage via DeGitX load balancer
    (front-end) via gRPC protocol defined by Gitaly component of GitLab.
  }
  \label{fig:gitlab-set}
\end{figure}

The communication layer of DeGit front-end and back-end is DeGit metadata: it consists of mapping
of repository hash to storage node locator, the front-end can be configured differently depends on
setup kind, it can query metadata from database, it can query lookup it via DHT\footnote{Distributed hash table}
or it can receive metadata updates broadcast from storage node peers in local network via UDP protocol,
see \ref{sec:metadata}.

When a client (e.g. GitLab-shell) write git data to the system (via \code{push}), the request is routing
by front-end load balancer to one of the storage replicas.
Storage back-end node starts leader election with other repository replicas and updates the log\footnote{Here: node log
or back-end log is distributed replicated log of state machine associated with certain git repository storage}
of consensus of repository holders (replicas), see \ref{sec:data}.

The system autmatically rebalances repository storage: when repository is not used actively for a
long time, the node can remove a repository from storage if 3 replicas of this repository exists on other nodes;
if some node has a lot of free space on storage, and storage of another node is almost full,
full node can transfer (move) some repositories to another node, and IOPS of storage device is also
a importance measure.

The system can accept new nodes and automatically fill it with replica repository and
move some repository to new node. Same for disconnecting: if node was disconnected from the system or crashed,
it creates additional replicas on nodes to have at least 3 replicas for each repository in a system.

\subsection{Protocol}
Main system protocols are:
\begin{description}
  \item[Location protocol] - unique node identity.
  \item[Network protocols] - nodes communication protocols.
  \item[Discovery protocol] - lookup of node real address by locator ID.
  \item[Metadata exchange protocol] - a mapping of git repository to storage back-end node.
  \item[Data exchange protocol] - Git objects and references exchange protocol,
    commands to add new objects and update references, linearizability guarantee.
\end{description}

\subsubsection{Locators}
\label{sec:locators}
Network addresses are not stable, back-end node may get network address via
Dynamic Host Configuration Protocol (DHTP), or node owner may move it from one server to another.
So we can't rely on network addresses when working with back-end nodes, we need some
overlay networks and unique identifiers for each node. To identify a node DeGit uses
public-key cryptography: node owner generates private and public key pair using one of the supported
crypto algorithms \todo{which exactly?}, these keys will identify back-end node uniquelly.
Node uses cryptographic hash \todo{choose algorithm} of public key as node locator ID,
%FIXME
% as reseearched in the ~\cite{securebaserouting}. Nodes uses locator IDs to introduce
itself to the system and build overlay network on top of real network. DeGit uses
Multihash\footnote{https://multiformats.io/multihash/} format to encode locator IDs.

Private and public keys can be used to sign requests to other nodes or to build
trusted discovery (see ~\ref{sec:discovery}) point in a system: when a new system is created,
administrator can create certification authority (CA) for issuing digital certificates for node
public keys, so each node will be able to verify certificate of any other node when
using seed list URLs or other peers exchange algorithms. Node instances uses locators to talk
with each other, it can find real network address of nodes using this locator ID.
Also, system uses locators for node-repository mapping (see ~\ref{sec:data}),
the item of mapping table consists of locator ID and unique repository name hash.

\subsubsection{Network}
\label{sec:network}

DeGitX uses TCP or UDP protocols for transport layer, TCP is used by default. It uses IPv4 or IPv6 for addressing,
it stores addresses in \href{https://github.com/multiformats/multiaddr}{multiaddr} format, for instance:
\code{/ipv4/1.2.3.4/udp/4444} evaluates to \code{4444} UDP port on \code{1.2.3.4} IPv4 address.
DeGitX uses a routing system (see \ref{sec:discovery}) to find addresses of nodes by locators.

\subsubsection{Discovery}
\label{sec:discovery}

Each node doesn't know network addresses of others by default, it knows only locator ID.
Different discovery techniques are used for node lookup, it can be configured independenly by node
administrator and they could be used together:

\begin{description}
  \item[LPD] - local peer discovery: each peer sends UDP messages with locator ID.
    It's cheap and fast for network layer (due to UDP messages); other system components,
    such as front-ends and back-ends receive these messages and update local cache of node locators.
    These approach dramatically improve lookup performance in local networks, e.g. when most of communications
    are performed in one local region, and all region nodes are connected via local networ.
    This protocol supposed to be used with other discovery techniques.
    For security reasons, in untrusted networks, the broadcast messages may be optionally signed with
    node private keys to be verified by the receiver using CA public certificate.
    For optimization reasons, this protocol is reused by metadata exchange protocol for
    repository hash table propagation (see \ref{sec:metadata}). \todo{lookup for researches}.
    There are 2 common ways to implement LDP: \href{http://bittorrent.org/beps/bep_0014.html}{bep 14} and \href{http://bittorrent.org/beps/bep_0026.html}{bep 26}.
    Bep 14 is SSDP-like style and Bep 26 is zeroconf style LDP. To implement Bep 26 each host is required to run a zeroconf service discovery daemon.
    There is a popular \href{https://github.com/grandcat/zeroconf}{go zeroconf implementation} that could be used.
    BitTorrent uses following multicast groups: A) \code{239.192.152.143:6771} (\href{https://tools.ietf.org/html/rfc2365#section-6.2}{org-local}) and B) \code{[ff15::efc0:988f]:6771} (\href{https://tools.ietf.org/html/rfc4291#section-2.5.7}{site-local}) for Bep 14 implementation.
    \begin{itemize}
      \item Site-Local scope is intended to span a single site.
      \item Organization-Local scope is intended to span multiple sites
      belonging to a single organization.
    \end{itemize}
    They've chosen such IPs because \code{239.192.0.0/14} is defined to be the IPv4 Organization Local Scope,
    and is the space from which an organization should allocate sub-
    ranges when defining scopes for private use.
    \code{ff15::efc0:988f} also comes from \href{http://tools.ietf.org/html/rfc4291#section-2.7}{IPv6 spec} and means:
    \begin{itemize}
      \item[--] FF == Multicast
      \item[--] 1 == ‘Flags' - where 1 indicates a non-permanently-assigned ("transient” or "dynamically" assigned) multicast address.
      \item[--] 5 == Site-Local scope
      \item[--] efc0:988f - the hex representation of 239.192.152.143
    \end{itemize}
    We could easily implement BEP 14 as it described and take \href{https://github.com/transmission/transmission/blob/7f147c65fb07a6baed3d079703ff0a31d1b1ca4c/libtransmission/tr-lpd.c}{this implementation} as an example.
  \item[Distributed DB] - network addresses could be propagated to the system using supplementary distributed
    database, e.g. etcd or others. On startup, the back-end node registers itself in database,
    and other system nodes (both front-ends and back-ends) uses this database for lookups. 
    It requires additional system configuration, but delegating some responsibilities to third-party services.
  \item[Seed hosts] - nodes can use other nodes as seed hosts as caching optimization, some nodes
    will be responsible for caching lookup results. In other words, these seed nodes represents similar abstraction
    as Content Delivery Network (CDN) in web caching.
  \item[DHT] - distributed hash tables uses different metrics to compare the distance of nodes in overlay network.
    For example Kademlia\footnote{pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf} uses
    \code{XOR} metric of node ID for distance measurement. It requires node IDs to be widely distributed -
    DeGit locators satisfies this requirements, being generated as cryptographic hash function from
    public key. In Kademlia lookup system each node stores references in K-buckets, where each K-bucket
    contains node addresses with same ID prefix.
\end{description}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \begin{umlcomponent}{Node}
        \umlbasiccomponent[name=locators]{Locators}
        \umlprovidedinterface[name=ilocators]{locators}

        \umlbasiccomponent[name=discovery, y=-4, x=-3]{Discovery}
        \umlVHVassemblyconnector{discovery}{ilocators}
      \end{umlcomponent}
      \umlport{Node}{355}
      \umlport{Node}{330}
      \umlport{Node}{315}
      \umldelegateconnector{discovery}{Node-355-port}
      \umldelegateconnector{discovery}{Node-330-port}
      \umldelegateconnector{discovery}{Node-315-port}
      \umlrequiredinterface[interface=DHT]{Node-355-port}
      \umlrequiredinterface[interface=LPD]{Node-330-port}

      \umlbasiccomponent[name=db, x=6, y=-5]{Database}
      \umlassemblyconnector[interface=DB]{Node-315-port}{db}
    \end{tikzpicture}
  \end{center}
  \caption{
    Doscovery protocols depends on Locators protocol,
    and uses multiple discovery interfaces: LPD on local network,
    regional database to speedup the lookup, DHT for global lookup
    in all system.
  }
  \label{fig:discovery-protocol}
\end{figure}

Depends on configuration node may use or may not some discovery protocols. They are performed
in well specified order: firstly, the node is lookup for locator ID in local cache, the cache is updated
by LPD broadcasts; in case if not found, the node is quering discovery database, then seed hosts, and DHT
as last resort.

The structure of discovery entries is a mapping of locator ID (which is cryptographic hash by design)
to network address in Multiaddr format (as described in network section: ~\ref{sec:network}).

Example of routing table:

\begin{tabular}{l | l}
  Locator ID & Node address \\ \hline
  \code{122041dd7b6443...0022ab11d2589a8} & \code{/ipv4/192.168.1.42/tcp/9031} \\
  \code{122041dd7b6443...0022ab11d2589a8} & \code{/ipv4/192.168.1.33/tcp/8011} \\
  \code{132052eb4dd19f...6f8c7d235eef5f4} & \code{/ipv4/172.18.11.22/tcp/9031} \\
\end{tabular}

\subsection{Metadata}
\label{sec:metadata}

DeGit peers doesn't know where to find repository storage by default,
to solve it, the system introduces metadata layer to exchange repository coordinates
to locator IDs. The structure of metadata is a many-to-many relation of
repository cryptographic hash to storage locator ID.

For instance, here is the example of metadata of two repositories located at two nodes,
repository \code{repo1} is located on both nodes \code{node1} and \code{node2},
the repository \code{repo2} is located only on node \code{node2}:

\begin{tabular}{l | l}
  Repository hash & Locator ID \\ \hline
  \code{hash(repo1)} & \code{locator(node1)} \\
  \code{hash(repo1)} & \code{locator(node2)} \\
  \code{hash(epo2)} & \code{locator(node2)} \\
\end{tabular}

The repository hash is encoded in Multihash format. Metadata exchange protocol partially reuses
discovery protocol for network optimizations, e.g. peers sends local broadcasts
with locator IDs and known repository hashes, discovery database may keep (if configured)
repository hash map to node locator IDs relations (see \ref{fig:repo-lookup-db}),
DHT keeps locator IDs as a value for repository hash keys \ref{fig:repo-lookup-dht},
it keeps all metadata of the whole system. The metadata lookup process is similar to discovery protocol:
firstly, peer is looking for metadata in local cache (populated with network broadcasts); then, it checks
region database; and as last resort, it performs query lookup for global DHT.

When a new node starts replicating some repository, it's synchronizing with other replicas first, and then
updates metadata asynchronously. It's how metadata is updated:
\begin{enumerate}
  \item A new node wants to replicate some repository
  \item The node finds current repository holders (replicas) in existing metadata and choose random node from this list
  \item A node sends the request to the node to add itself to the replica list
  \item Receiver node starts leader election and updates node log to add new node to the replica list
  \item The consensus accepts new node and stores it in local persistant storage
  \item The leader notifies a new node that it becomes a part of the replicas and consider it when consensus is required
  \item New node replicates the state of consensus and holds the repository and sends UDP broadcast to
    local peers on success
  \item Leader node propagates replicas change in metadata storages asynchronously: updates DHT or database storage
    (as configured)
\end{enumerate}

Repository mapping could be stored in distributed hash table (e.g. Kademlia),
database cache or broadcasted via local network broadcasts.
E.g. for multi-region cluster setup, the front-end load balancer may look for node in local cache
populated by local network broadcasts, then check region database where all repositories in same region are
registered, and if not found perform DHT query lookup in different regions,
see Figures \ref{fig:repo-lookup-db} \ref{fig:repo-lookup-dht}.

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \node[block] (db) {DB};
      \node[block, below=of db, xshift=-3cm] (fe) {Front-end};
      \node[O, below=of db, xshift=3cm] (Nx) {N$_{x}$};

      \draw[<->] (fe) -- node {1} (db);
      \draw[<->] (fe) -- node {2} (Nx);
    \end{tikzpicture}
  \end{center}
  \caption{
    Repository lookup in database:
    DB - Database with metadata for current region.
    1 step - front-end load balancer query database for repository metadata.
    2 step - front-end access node N$_{x}$ with required repository.
  }
  \label{fig:repo-lookup-db}
\end{figure}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \matrix[row sep=10mm](feg){
        \node[block] (fe) {Front-end}; \\
      };
      \matrix[row sep=3mm, column sep=5mm, right=of feg](r1){
        \node[O] (r1n1) {N$_{r1,1}$}; \\
        \node[O] (r1n2) {N$_{r1,2}$}; \\
        \node[O] (r1n3) {N$_{r2,3}$}; \\
        \node[O] (r1nN) {N$_{r1,n}$}; \\
      };
      \node[fit=(r1)(feg), draw, dashed, label={[right=0cm, above=3cm]1cm:R$_{1}$}](r1g){};
      \matrix[row sep=3mm, column sep=5mm, right=of r1](r2){
        \node[O] (r2n1) {N$_{r2,1}$}; \\
        \node[O] (r2n2) {N$_{r2,2}$}; \\
        \node[O] (r2nN) {N$_{r2,n}$}; \\
      };
      \node[fit=(r2), draw, dashed, label={[right=0cm, above=3cm]1cm:R$_{2}$}](r2g){};
      \matrix[row sep=3mm, column sep=5mm, right=of r2](r3){
        \node[O] (r3n1) {N$_{r3,1}$}; \\
        \node[O] (r3n2) {N$_{r3,2}$}; \\
        \node[O] (r3n3) {N$_{r3,3}$}; \\
        \node[O] (r3nN) {N$_{r3,n}$}; \\
      };
      \node[fit=(r3), draw, dashed, label={[right=0cm, above=3cm]1cm:R$_{3}$}](r3g){};
      \draw[-latex] (fe) -- (r1n2);
      \draw[-latex] (r1n2) -- (r2n1);
      \draw[-latex] (r2n1) -- (r3n3);
    \end{tikzpicture}
  \end{center}
  \caption{
    Repository lookup via DHT:
    Front-end in Region R$_{1}$ unable to find repository located on node N$_{r3,3}$ in same region,
    but it query the nearest known node N$_{r2,1}$, the node N$_{r2,1}$ knows where repository is located
    and redirects the query to node N$_{r3,3}$ with repository.
  }
  \label{fig:repo-lookup-dht}
\end{figure}

\subsection{Security}
\label{subsec:security}
A small fraction of malicious nodes can prevent correct message delivery throughout the overlay.

Such nodes may mis-route, corrupt, or drop messages and routing information.
Additionally, they may attempt to assume the identity of other nodes and corrupt or delete objects
they are supposed to store on behalf of the system.

All attacks based on presence of malicious nodes.
There are 2 ways:
\begin{itemize}
  \item[$-$] Implement techniques that allow nodes to join the overlay, to maintain routing state, and to forward messages securely in the presence of malicious nodes.
  \item[$-$] Make the appearance of malicious nodes impossible.
\end{itemize}

Degitx is not an open peer-to-peer system
where resource pooling without preexisting trusted relationships is possible.
It means that untrusted nodes aren't allowed to join
and all members of the network are trusted not to cheat.

To be sure that node is trusted, it's nodeId certificates should be signed by trusted CAs.
Then each node rejects all unsigned requests.

Certified nodeIds work well when nodes have fixed nodeIds.
This condition is met while Node uses cryptographic hash of public key as nodeId.

These certificates give the overlay a public key infrastructure,
suitable for establishing encrypted and authenticated channels between nodes.
Nodes with valid nodeId certificates can join the overlay, route messages,
and leave repeatedly without involvement of the CAs.

When the membership of a peer-to-peer system is constraint and all nodes are trusted as in DeGitX,
%FIXME
% CAs could solve security issues, because, according to the ~\cite{securerouting} research, all attacks are based on malicious nodes presence.

\subsection{Git data exchange}
\label{sec:data}
Consensus could be implemented using \emph{\href{https://raft.github.io/raft.pdf}{Raft}} or
\emph{\href{http://www.cs.yale.edu/homes/aspnes/pinewiki/Paxos.html}{Paxos}} algorithm.

\todo{search for more algorithms}.

\include{explanation}

\section{Requirements}
\label{sec:requirements}

\subsection{Features}
\label{sec:features}

The most critical
\href{https://en.wikipedia.org/wiki/Non-functional_requirement}{Non-functional requirements}
are:

\begin{description}
  \item[Read scalability]
    The solution should scale out the read capacity of a system, each region should be able
    to access repository using most available replica node.
  \item[Strong consistency]
    All? (\todo{discuss, maybe not all but the majority of replicas})
    active replica repositories should be synchronized on updates in any node
    with immediate consistency.
  \item[Durability]
    The system must have enough replicas to recover itself in case of corruption.
    Corrupted repository could be responsible for recovering itself using replica nodes.
  \item[Self management (rename?)]
    Each node performs cleanup when needed (\code{git gc}) and may remove replica
    from storage on read inactivity.
    A node should be able to find and synchronize new repository on read,
    after that it should be up to date on new updates.
  \item[Maintainability]
    Node administrator can change the storage, and perform data migration from one storage
    to another.
    Repository administrators are able to add or delete node for new region and
    get all nodes status for repository.
  \item[Auditability]
    Node doesn't perform access control operations, but logs all
    requests with identity and performed operation.
  \item[Analytics]
    Node collects statistics for each repository and usage metrics, such as
    push and pull operations, etc. The system keeps the whole statistics about
    nodes, e.g. how many nodes contains each repository, the state of nodes, etc.
\end{description}

\section{Compare to other solutions}

These products are similar to DeGit by some aspects:
\begin{description}
  \item[Spokes]
    GitHub announced \href{https://github.blog/2016-04-05-introducing-dgit/}{DGit}
    in 2016 (renamed to \href{https://github.blog/2016-09-07-building-resilience-in-spokes/}{Spokes})
    where they \href{https://github.blog/2016-09-07-building-resilience-in-spokes/#defining-resilience}{pay attention}
    to the consistency:
    \begin{quote}
      Spokes puts the highest priority on consistency and partition tolerance.
      In worst-case failure scenarios, it will refuse to accept writes that it cannot commit,
      synchronously, to at least two replicas.
    \end{quote}
    It's a proprietary software that can't be used for free and the source code is closed.
    Spokes papers claims that it pays attention to consistency, but on the
    \href{https://www.youtube.com/watch?v=DY0yNRNkYb0}{conference talk} they mentioned that
    it's rarely possible to break the consistency which requires manual intervention.
    Therefore the approach of distributed system design used by Spokes is not suitable for open
    source project, where maintainance team doesn't exist.
  \item[Gitaly]
    \href{https://docs.gitlab.com/ee/README.html}{Gitlab} has
    \href{https://docs.gitlab.com/ee/administration/gitaly/}{Gitaly} service which provides
    \code{gRPC} API for Gitlab website and git-ssh proxy to perform all git operations via API.
    It's \href{https://gitlab.com/gitlab-org/gitaly}{open source} component.
    Gitaly proposed new design for service which claims to provide
    \href{https://gitlab.com/gitlab-org/gitaly/-/blob/master/doc/design\_ha.md\#strong-consistency-design}{strong concistency}
    but in fact it doesn't provide linearability of commands in system \todo{arguments and proves}.
    Futhermore, it's possible that GitLab may change HA licensing \todo{find cases},
    or restrict HA support \href{https://news.ycombinator.com/item?id=21437334}{based on country residence}.
  \item[JGit]
    \href{https://www.eclipse.org/jgit/}{Jgit} is a Java git server created by \href{https://www.eclipse.org/}{Eclipse}.
    Google \href{https://www.eclipse.org//lists/jgit-dev/msg03073.html}{contributed} to this project with Ketch module:
    \begin{quote}
      Git Ketch is a multi-master Git repository management system. Writes
      (such as git push) can be started on any server, at any time. Writes
      are successful only if a majority of participant servers agree.
      Acked writes are durable against server failure, due to a majority of
      the participants storing all required objects.
    \end{quote}
    But this is the only place where Ketch is mentioned. \todo{Analyze source code of Ketch module}.
  \item[IPFS]
    \href{https://ipfs.io/}{IPFS} is not exactly distributed git repository project, but has similar ideas
    and cound be helpfull for us. \todo{analyze IPFS project}.
  \item[brig]
    \todo{analyze the project} \href{https://github.com/sahib/brig}{brig}.
\end{description}

\subsection{Functional Requirements}
\label{sec:nfr}

The most important \href{https://en.wikipedia.org/wiki/Functional_requirement}{functional requirements} are:

\begin{description}
  \item[Front end]
    The system potentically may have different kinds of front-ends,
    but it's required to support \href{https://grpc.io/}{gRPC}
    of \href{https://about.gitlab.com/}{GitLab} to integrate the system
    into GitLab service and replace
    \href{https://docs.gitlab.com/ee/administration/gitaly/}{Gitaly}.
  \item[Back end]
    Each node may be connected to different types of storage for git repos,
    but it's required to support file-system storage.
\end{description}

\subsection{Expected Metrics}
\label{ref:metrics}

In a large enterprise it is expected to have the following
numbers, in terms of load, size, and speed:

\begin{tabular}{ll}
  Repositories & 2M \\
  Active users & 100K/day \\
  Merges & 100K/day \\
  Fetches & 15M/day, 15K/m - peak \\
  Push & 200K/day \\
  Traffic - download & 200Tb/day \\
  Traffic - update & 250Gb/day \\
\end{tabular}

% \include{appendix-a}

\section{References}
\label{ref:references}

Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial, FRED B. SCHNEIDER Department of Computer Science, Cornell University, Ithaca, New York 14853 U.S.A.

Replication Management using the State Machine Approach, Fred B. Schneider Department of Computer Science Cornell University Ithaca, NewYork 14853 U.S.A.

Building resilience in Spokes, Patrick Reynolds, https://github.blog/2016-09-07-building-resilience-in-spokes/

Kademlia: A Peer-to-peer information system based on the XOR Metric, Petar Maymounkov and David Mazieres New Yourk University

S/Kademlia: A Practicable Approach Towards Secure Key-Based RoutingIngmar Baumgart and Sebastian MiesInstitute of TelematicsUniversit at Karlsruhe (TH)D–76128 Karlsruhe, Germany

IPFS - Content Addressed, Versioned, P2P File System (DRAFT 3) Juan Benet

% \printbibliography
\end{document}
